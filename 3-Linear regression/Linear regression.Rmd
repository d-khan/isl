---
title: "Linear Regression"
author: "Danish Khan"
date: '2017-11-23'
output:
  html_document:
    fig_width: 7
    fig_height: 6
    fig_caption: true
    toc: true
    toc_float: true
    toc_depth: 4
    theme: journal
---

Linear regression is a simple useful tool for predicting a quantitative response. Linear regression is still a useful and widely used statistical learning method. 

Recall the Advertising data as shown below in Figure displays sales (in thousands of units) for a particular product as a function of advertis- ing budgets (in thousands of dollars) for TV, radio, and newspaper media. Suppose that in our role as statistical consultants we are asked to suggest, on the basis of this data, a marketing plan for next year that will result in high product sales. What information would be useful in order to provide such a recommendation? 

![_Figure: The Advertising data set. The plot displays sales, in thousands of units, as a function of TV, radio, and newspaper budgets, in thousands of dollars, for 200 different markets. In each plot we show the simple least squares fit of sales to that variable, as described in Linear Regression chapter. In other words, each blue line represents a simple model that can be used to predict sales using TV, radio, and newspaper, respectively._](2-1.png)

There are few important questions that we might seek to address:

* Is there a relationship between advertising budget and sales?  
* How strong is the relationship between advertising budget and sales?  
* Which media contribute to sales?  
* How accurately can we estimate the effect of each medium on sales?  
* How accurately can we predict future sales?  
* Is the relationship linear?  
* Is there synergy among the advertising media?  

## Simple linear regression  
Simple linear regression is a very straightforward approach for predicting a quantitative response $Y$ on the basis of a single predictor variable $X$. The relationship can be written as: $$Y \approx \beta_0 + \beta_1X$$.

In this advertising example, $Y$ represent sales and $X$ represents TV or radio or newspaper advertising. Mathematically the equation can be written as: $$sales \approx \beta_0 + \beta_1 \times TV$$

where $\beta_0$ is intercept and $\beta_1$ is slope. These $\beta_0$ and $\beta_1$ are unknown constants these are known as model _coefficients_ or _parameters_.

When training data is used to estimate $\beta_0$ and $\beta_1$ and to predict future sales $Y$ then a hat symbol is used to denote the estimated value for an unknown parameter or coefficient, or to denote the predicted value of the response. The equation can be written as: $$y = \hat \beta_0 + \hat \beta_1x$$

## Estimating the coefficients
In practice $\beta_0$ and $\beta_1$ are unknown. Our goal is to estimate $\hat\beta_0$ and $\hat\beta_1$. Mathematically the equation can be written as $y_i \approx \beta_0 + \beta_1x_i$ for $i = 1,...,n$. In the Advertising example, $x$ could be TV, radio or newspaper; $y$ is sales and $n$ could be different markets. In order to find coefficient estimates as close to actual coefficients, the most common approach is _least squares_.

![_Figure: For the Advertising data, the least squares fit for the regression of sales onto TV is shown. The fit is found by minimizing the sum of squared errors. Each grey line segment represents an error, and the fit makes a compromise by averaging their squares. In this case a linear fit captures the essence of the relationship, although it is somewhat deficient in the left of the plot._](3-1.png)

The prediction of Y on X based on the ith value can be written as: $\hat y_i = \hat \beta_0 + \hat \beta_1x_i$ where $i = 1,...,n$ data points. The difference between the observed response and estimated response can be written as: $e_i = y_i - \hat y_i$. The sum of all errors for all data points can be written as: $$RSS = e_1^2+e_2^2+...+e_n^2$$

The _least squares coefficient estimates_ for simple linear regression can be calculated as: 
$$\hat \beta_0 = \bar y - \hat \beta_1 \bar x$$ 
$$\hat \beta_1 = \frac{\sum_{i=1}^n(x_i - \bar x)(y_i - \bar y)}{\sum_{i=1}^n(x_i - \bar x)^2}$$

where $\bar y\equiv \frac{1}{n} \sum_{i=1}^n y_i$ and $\bar x \equiv \frac{1}{n} \sum_{i=1}^n x_i$ are the sample means. $\equiv$ means _as defined as_ or _by definition_.

Based on the above figure, $\hat \beta_0 = 7.03$ which is intercept. This means that when advertising is 0, the number of units sold is around 7000. $\hat \beta_1 = 0.0475$ is called slope and this means that every $1000 spent on TV advertising, the product sale is around 47.5 additional units.  

## Accessing the accuracy of the coefficient estimates  

The true relationship between $X$ and $Y$ is $$Y = f(X) + \epsilon$$ where $f$ is an unknown function and $\epsilon$ is a mean-zero random error term. If we assume that $f$ is linear then the equation becomes: $$Y=\beta_0 + \beta_1X + \epsilon$$

Here $\beta_0$ is the intercept term that is, the expected value of Y when X = 0, and $\beta_1$ is the slope, the average increase in $Y$ associated with a one-unit increase in $X$.

Note that the true relationship probably may not linear and there may be other variables that cause variation in Y, and there may be measurement errors. That is why an error term is introduced in the equation $Y=\beta_0 + \beta_1X + \epsilon$ which is independent of $X$.

The equation $Y=\beta_0 + \beta_1X + \epsilon$ gives the _population regression line_ which is the best linear approximation to the true relationship between $X$ and $Y$. This is also called _true population regression line_.

The _least squares line_ which is defined as $\hat y = \hat \beta_0 + \hat \beta_1x$ gives close estimation to the _population regression line_.

Based on the figure shown below, 100 random $X$s and corresponding 100 $Y$s are generated from the model $Y = 2+3X+\epsilon$ where $\epsilon$ was generated from a normal distribution with mean zero. The red line shows the true relationship with no error $f(X)=2+3X$, while the blue line is the least squares estimate based on the observed data. In real applications, we have access to a set of observations from which we can compute the least square line but the true population regression line is unobserved. In other words, the true relationship for real data is generally not known. In the right-hand panel of the Figure shown above we have generated ten different data sets from the model given by $Y = 2+3X+\epsilon$ and plotted the corresponding ten least squares lines. Notice that different data sets generated from the same true model result in slightly different least squares lines, but the unobserved population regression line does not change.

![Figure: A simulated data set. Left: The red line represents the true relationship, $f(X)=2+3X$, which is known as the population regression line. The blue line is the least squares line; it is the least squares estimate for $f(X)$ based on the observed data, shown in black. Right: The population regression line is again shown in red, and the least squares line in dark blue. In light blue, ten least squares lines are shown, each computed on the basis of a separate random set of observations. Each least squares line is different, but on average, the least squares lines are quite close to the population regression line.](3-3.png)

With one dataset, why there are two lines and why these are different? This is because the red line represents the entire data set (population) and the blue line represents the part of the data set (sample). Suppose we want to know mean height of every male and female on this planet between the age of 20-30. This is called _population mean_ which is represented by $\mu$. However it is not possible to know the height ($Y$) of every single male and female but we can always estimate mean height ($Y$) based on the available number of males and females ($n$ observations). This is called _sample mean_ which is represented by $\bar y$. If we estimate population mean $\hat \mu$ by using sample mean $\bar y$ then mathematically the equation can be written as $\hat \mu = \bar y$ where $$\bar y = \frac {1}{n}{\sum_{i=1}^n y_i}$$

If we use sample mean to estimate $\hat \mu$, this estimate is _unbiased_ which means that on average we expect $\hat \mu$ to equal $\mu$.  

__What unbiased means?__ It means that when small number of observations (samples) are used to estimate population mean then $\hat \mu$ might underestimate or overestimate $\mu$. But if we use a huge number of observations (samples) then $\hat \mu$ would exactly equal $\mu$. 

The property of unbiasedness holds for the least squares coefficient estimates as well: if we estimate $\beta_0$ and $\beta_1$ on the basis of a particular data set, then our estimates wonâ€™t be exactly equal to $\beta_0$ and $\beta_1$. But if we could average the estimates obtained over a huge number of data sets, then the average of these estimates would be spot on! In fact, we can see from the right hand panel of the Figure shown above that the average of many least squares lines, each estimated from a separate data set, is pretty close to the true population regression line.  

_How accurate is the sample mean $\hat \mu$ as an estimate of $\mu$?  



