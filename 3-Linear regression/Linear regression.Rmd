---
title: "Linear Regression"
author: "Dr Danish Khan"
date: '2017-11-23'
output:
  html_document:
    fig_width: 7
    fig_height: 6
    fig_caption: true
    toc: true
    toc_float: true
    toc_depth: 4
    theme: journal
---

Linear regression is a simple useful tool for predicting a quantitative response. Linear regression is still a useful and widely used statistical learning method. 

Recall the Advertising data as shown below in Figure displays sales (in thousands of units) for a particular product as a function of advertis- ing budgets (in thousands of dollars) for TV, radio, and newspaper media. Suppose that in our role as statistical consultants we are asked to suggest, on the basis of this data, a marketing plan for next year that will result in high product sales. What information would be useful in order to provide such a recommendation? 

![_Figure: The Advertising data set. The plot displays sales, in thousands of units, as a function of TV, radio, and newspaper budgets, in thousands of dollars, for 200 different markets. In each plot we show the simple least squares fit of sales to that variable, as described in Linear Regression chapter. In other words, each blue line represents a simple model that can be used to predict sales using TV, radio, and newspaper, respectively._](2-1.png)

There are few important questions that we might seek to address:

* Is there a relationship between advertising budget and sales?  
* How strong is the relationship between advertising budget and sales?  
* Which media contribute to sales?  
* How accurately can we estimate the effect of each medium on sales?  
* How accurately can we predict future sales?  
* Is the relationship linear?  
* Is there synergy among the advertising media?  

## Simple linear regression  
Simple linear regression is a very straightforward approach for predicting a quantitative response $Y$ on the basis of a single predictor variable $X$. The relationship can be written as: $$Y \approx \beta_0 + \beta_1X$$.

In this advertising example, $Y$ represent sales and $X$ represents TV or radio or newspaper advertising. Mathematically the equation can be written as: $$sales \approx \beta_0 + \beta_1 \times TV$$

where $\beta_0$ is intercept and $\beta_1$ is slope. These $\beta_0$ and $\beta_1$ are unknown constants these are known as model _coefficients_ or _parameters_.

When training data is used to estimate $\beta_0$ and $\beta_1$ and to predict future sales $Y$ then a hat symbol is used to denote the estimated value for an unknown parameter or coefficient, or to denote the predicted value of the response. The equation can be written as: $$y = \hat \beta_0 + \hat \beta_1x$$

## Estimating the coefficients
In practice $\beta_0$ and $\beta_1$ are unknown. Our goal is to estimate $\hat\beta_0$ and $\hat\beta_1$. Mathematically the equation can be written as $y_i \approx \beta_0 + \beta_1x_i$ for $i = 1,...,n$. In the Advertising example, $x$ could be TV, radio or newspaper; $y$ is sales and $n$ could be different markets. In order to find coefficient estimates as close to actual coefficients, the most common approach is _least squares_.

![_Figure: For the Advertising data, the least squares fit for the regression of sales onto TV is shown. The fit is found by minimizing the sum of squared errors. Each grey line segment represents an error, and the fit makes a compromise by averaging their squares. In this case a linear fit captures the essence of the relationship, although it is somewhat deficient in the left of the plot._](3-1.png)

The prediction of Y on X based on the ith value can be written as: $\hat y_i = \hat \beta_0 + \hat \beta_1x_i$ where $i = 1,...,n$ data points. The difference between the observed response and estimated response can be written as: $e_i = y_i - \hat y_i$. The sum of all errors for all data points can be written as: $$RSS = e_1^2+e_2^2+...+e_n^2$$

The _least squares coefficient estimates_ for simple linear regression can be calculated as: 
$$\hat \beta_0 = \bar y - \hat \beta_1 \bar x$$ 
$$\hat \beta_1 = \frac{\sum_{i=1}^n(x_i - \bar x)(y_i - \bar y)}{\sum_{i=1}^n(x_i - \bar x)^2}$$

where $\bar y\equiv \frac{1}{n} \sum_{i=1}^n y_i$ and $\bar x \equiv \frac{1}{n} \sum_{i=1}^n x_i$ are the sample means. $\equiv$ means _as defined as_ or _by definition_.

Based on the above figure, $\hat \beta_0 = 7.03$ which is intercept. This means that when advertising is 0, the number of units sold is around 7000. $\hat \beta_1 = 0.0475$ is called slope and this means that every $1000 spent on TV advertising, the product sale is around 47.5 additional units.  

## Accessing the accuracy of the coefficient estimates  

The true relationship between $X$ and $Y$ is $$Y = f(X) + \epsilon$$ where $f$ is an unknown function and $\epsilon$ is a mean-zero random error term. If we assume that $f$ is linear then the equation becomes: $$Y=\beta_0 + \beta_1X + \epsilon$$

Here $\beta_0$ is the intercept term that is, the expected value of Y when X = 0, and $\beta_1$ is the slope, the average increase in $Y$ associated with a one-unit increase in $X$.

Note that the true relationship probably may not linear and there may be other variables that cause variation in Y, and there may be measurement errors. That is why an error term is introduced in the equation $Y=\beta_0 + \beta_1X + \epsilon$ which is independent of $X$.

The equation $Y=\beta_0 + \beta_1X + \epsilon$ gives the _population regression line_ which is the best linear approximation to the true relationship between $X$ and $Y$. This is also called _true population regression line_.

The _least squares line_ which is defined as $\hat y = \hat \beta_0 + \hat \beta_1x$ gives close estimation to the _population regression line_.

Based on the figure shown below, 100 random $X$s and corresponding 100 $Y$s are generated from the model $Y = 2+3X+\epsilon$ where $\epsilon$ was generated from a normal distribution with mean zero. The red line shows the true relationship with no error $f(X)=2+3X$, while the blue line is the least squares estimate based on the observed data. In real applications, we have access to a set of observations from which we can compute the least square line but the true population regression line is unobserved. In other words, the true relationship for real data is generally not known. In the right-hand panel of the Figure shown above we have generated ten different data sets from the model given by $Y = 2+3X+\epsilon$ and plotted the corresponding ten least squares lines. Notice that different data sets generated from the same true model result in slightly different least squares lines, but the unobserved population regression line does not change.

![Figure: A simulated data set. Left: The red line represents the true relationship, $f(X)=2+3X$, which is known as the population regression line. The blue line is the least squares line; it is the least squares estimate for $f(X)$ based on the observed data, shown in black. Right: The population regression line is again shown in red, and the least squares line in dark blue. In light blue, ten least squares lines are shown, each computed on the basis of a separate random set of observations. Each least squares line is different, but on average, the least squares lines are quite close to the population regression line.](3-3.png)

With one dataset, why there are two lines and why these are different? This is because the red line represents the entire data set (population) and the blue line represents the part of the data set (sample). Suppose we want to know mean height of every male and female on this planet between the age of 20-30. This is called _population mean_ which is represented by $\mu$. However it is not possible to know the height ($Y$) of every single male and female but we can always estimate mean height ($Y$) based on the available number of males and females ($n$ observations). A reasonable estimate to population mean $\mu$ can be written as $\hat \mu = \bar y$ where $$\bar y = \frac {1}{n}{\sum_{i=1}^n y_i}$$ is the sample mean. In a nutshell, population mean is represented by $\mu$ and sample mean is represented by $\hat \mu$.

If we use sample mean to estimate $\hat \mu$, this estimate is _unbiased_ which means that on average we expect $\hat \mu$ to equal $\mu$.  

__What unbiased means?__ It means that when small number of observations (samples) are used to estimate population mean then $\hat \mu$ might underestimate or overestimate $\mu$. But if we use a huge number of observations (samples) then $\hat \mu$ would exactly equal $\mu$. 

The property of unbiasedness holds for the least squares coefficient estimates as well: if we estimate $\beta_0$ and $\beta_1$ on the basis of a particular data set, then our estimates won’t be exactly equal to $\beta_0$ and $\beta_1$. But if we could average the estimates obtained over a huge number of data sets, then the average of these estimates would be spot on! In fact, we can see from the right hand panel of the Figure shown above that the average of many least squares lines, each estimated from a separate data set, is pretty close to the true population regression line.  

__How accurate is the sample mean $\hat \mu$ as an estimate of $\mu$?__ The answer lies in computing the _standard error of $\hat \mu$, written as SE$(\hat \mu)$. The formula of SE is: $$Var(\hat \mu)=SE(\hat \mu)^2=\frac{\sigma^2}{n}$$ where $\sigma$ is the standard deviation. SE tells the average amount that the estimate $\hat \mu$ differs from the actual value of $\mu$. Similarly we can calculate how close $\hat \beta_0$ and $\hat \beta_1$ are to the true values $\beta_0$ and $\beta_1$. SE of $\hat \beta_0$ and $\hat \beta_1$ can be written as: $$SE(\hat\beta_0)^2 = \sigma^2\Big[\frac{1}{n}+\frac{\bar x^2}{\sum_{i=1}^n(x_i-\bar x^2)}\Big]$$ $$SE(\hat\beta_1)^2 = \sigma^2\Big[\frac{\bar \sigma^2}{\sum_{i=1}^n(x_i-\bar x)^2}\Big]$$

where $\sigma^2 = Var(\epsilon)$. Generally $\sigma^2$ is not known but can be estimated from the data set. The estimation of $\sigma$ is known as the _residual standard error_ and the formula is $$RSE = \sqrt{\dfrac{RSS}{n-2}}$$

### Confidence interval

Standard errors can be used to calculate _confidence intervals_. CI can be defined as "range of values such that with 95% probability, the range will contain the true unknown value of the parameter". CI is defined in terms of lower and upper limits computed from the sample data. In other words, CI tells you how confident you can be that the results from a poll or survey reflect what you would expect to find if it were possible to survey the entire population.

> __95% confidence interval explained__  
The terms _confidence level_ and _confidence interval_ are often confused. A 95% confidence level means is that if the survey or experiment were repeated, 95 percent of the time the data would match the results from the entire population. Sometimes you just can’t survey everyone because of time or cost (think about how much it would cost to do a telephone survey of over 300 million Americans!). Therefore, you take a sample of the population. Having a 95% confidence level means that you’re almost certain your results are the same as if you had surveyed everyone.  
A 95% _confidence interval_ gives you a very specific set of numbers for your confidence level. For example, let’s suppose you were surveying a local school to see what the student’s state test scores are. You set a 95% confidence level and find that the 95% confidence interval is (780,900). That means if you repeated this over and over, 95 percent of the time the scores would fall somewhere between 780 and 900.

![Figure: Confidence interval at 95%](ci95.png)

Above text is taken from http://www.statisticshowto.com/probability-and-statistics/confidence-interval/ on Nov 27 2017.

***

For linear regression, the 95% confidence interval for $\beta_0$ is $\hat \beta_0 \pm 2 \times SE(\hat \beta_1)$ and for $\beta_1$ is $\hat \beta_1 \pm 2 \times SE(\hat \beta_1)$

The constant (also called z-value) which is 2 (see equation above) at 95% confidence interval is rounded off. The z-value for different confidence intervals are pre-calculated and are shown in the table below.

Confidence level  |     z
---------------   |   -----
0.70              |   1.04
0.75              |   1.15
0.80              |   1.28
0.85              |   1.44
0.90              |   1.645
0.92              |   1.75
0.95              |   1.96
0.96              |   2.05
0.98              |   2.33
0.99              |   2.58

The 95% confidence interval for the advertising data for $\beta_0$ is [6.130,7.935] and $\beta_1$ is [0.042,0.053]. The $\beta_0$ is an intercept which means that when X=0 (with no advertising), the sales on averahe will fall somewhere between 6,130 and 7,930 units. Remember that the y-axis unit is in thousands. In terms of $\beta_1$, for each $1,000 increase in TV advertising, there will be an average increase in sales of between 42 and 53 units. Remember the unit of x-axis is in thousands.

Calculating 95% confidence interval based on the equation: $\beta_0$ is $\hat \beta_0 \pm 2 \times SE(\hat \beta_1)$ and for $\beta_1$ is $\hat \beta_1 \pm 2 \times SE(\hat \beta_1)$

Based on the table below, we know $\hat \beta_0$, $\hat \beta_1$ and standard errors:

According to the above equations:  
$\beta_0 = 7.0325 \pm 2 \times 0.4578$ gives [6.1169, 7.9481]  
$\beta_1 = 0.0475 \pm 2 \times 0.0027$ gives [0.0421, 0.0529]  

![Table:Coefficients of the least square model, standard errors, t-staistic and p-value. For the Advertising data, coefficients of the for the regression of number of units sold on TV advertising of $1,000 in the TV advertising budget is associated with an around 50 units (Recall that the sales variable is in thousands of units, and the TV variable is in thousands of dollars). Moreover at zero TV advertising budget, the number of units sold is approximately 7,000](Table 3-1.png)

### Hypothesis test
Standard errors can be used to perform hypothesis test on the coefficients. The most common hypothesis test are _null hypothesis_ and _alternative hypothesis_. Null hypothesis can be defined as $H_0$ which means no relationship between X and Y and alternative hypothesis $H_a$ which means there is some relationship between X and Y. In hypothesis test only coefficient $\beta_1$ is tested and if $\beta_1 = 0$ then null hypothesis ($H_0$) is confirmed and if $\beta_1 \ne 0$ then alternative hypothesis ($H_a$) is confirmed. In linear regression when $\beta_1$ becomes zero then the equation has only $Y=\beta_0 + \epsilon$

To test the null hypothesis, we need to determine whether $\hat \beta_1$, our estimate for $\beta_1$, is sufficiently far from zero that we can be confident that $\beta_1$ is non-zero. How far is far enough? This of course depends on the accuracy of $\hat\beta_1$ that is, it depends on SE($\hat \beta_1$). If SE($\hat \beta_1$) is small, then even relatively small values of $\hat \beta_1$ may provide strong evidence that $\beta_1 \ne 0$, and hence that there is a relationship between X and Y . In contrast, if SE($\hat\beta_1$) is large, then $\hat\beta_1$ must be large in absolute value in order for us to reject the null hypothesis.

In practice we compute _t-statistic_ given by $$t = \frac{\hat\beta_1-0}{SE(\hat\beta_1)}$$ which measures the number of standard deviations that $\hat \beta_1$ is away from zero. If there really is no relationship between X and Y , then we expect that the above equation will have a t-distribution with n − 2 degrees of freedom. 

The probability of p-value can be interpret as: a small p-value indicates that it is unlikely to observe such a substantial association between the predictor and the response due to chance, in the absence of any real association between the predictor and the response. In other words, small p-value means that we can infer that there is an association between the predictor and the response and thus reject the _null hypothesis_. Typical p-value cutoffs for rejecting the null hypothesis are 5 or 1 %. When n = 30, these correspond to t-statistics of around 2 and 2.75, respectively (see above table).

The above tables provides details of the least squares model for the regression of number of units sold on TV advertising budget for the Advertising data. Notice that the coefficients for $\hat \beta_0$ and $\hat \beta_1$ are very large relative to their standard errors, so the t-statistics are also large; the probabilities of seeing such values if $H_0$ is true are virtually zero. Hence we can conclude that $\beta_0 \ne 0$ and $\beta_1 \ne 0$.

__In a nutshell the following can be concluded:__  

* When $\beta_0 \ne 0$ and $\beta_1 \ne 0$: There is a relationship between X & Y.   
* When $\beta_0 = 0$ and $\beta_1 \ne 0$: If the intercept is zero (no intercept), the resulting model implies that the response must be exactly zero when all the predictors are set to zero.  
* When $\beta_0 \ne 0$ and $\beta_1 = 0$: There is no relationship between X & Y.  

## Accessing the accuracy of the model
Now we have rejected null hypothesis and in favor of the alternative hypothesis, now we want to quantify the extent to which the model fits the data. In other words we want to know how accurate the model is. The quality of linear regression fit is typically assessed using two related quantities: __residual standard error (RSE)__ and the __$R^2$__ statistic.

Quantity                  |   Value
------------------------  |   -----
Residual standard error   |   3.26
$R^2$                     |   0.612
F-statistic               |   312.1  

Table -> For the Advertising data, more information about the least squares model for the regression of number of units sold on TV advertising budget.

###Residual standard error  
The __RSE__ is an estimate of the standard deviation of $\epsilon$. In other words, it is the average amount that the response will deviate from the true regression line. 

The RSE is defined as: $$RSE = \sqrt{\dfrac{1}{n-2} RSS}$$ and RSS is defined as $$RSS = \sum_{i=1}^n(y_i-\hat y_i)^2$$

So RSE finally becomes $$RSE = \sqrt{\dfrac{1}{n-2}\sum_{i=1}^n(y_i-\hat y_i)^2}$$ 

Based on the table above, the RSE is 3.26. This means that even if the model were correct and the true values of the unknown coefficients $\beta_0$ and $\beta_1$ were known exactly, any prediction of sales on the basis of TV advertising would still be off by about 3,260 units on average. Of course, whether or not 3,260 units is an acceptable prediction error depends on the problem context. In the advertising data set, the mean value of sales over all markets is approximately 14,000 units, and so the percentage error is 3,260/14,000 = 23%.

If the predictions obtained using the model are very close to the true outcome which is $\hat y_i \approx y_i$ for $i$ = 1,...,$n$ then the RSE will be small and we can conclude that the model fits the data very well. On the other hand, if $\hat y_i$ is very far from  $y_i$ for one or more observations, then the RSE may be large which indicates that the model does not fit very well.

###$R^2$ statistic
The RSE provides an absolute measure of lack of fit of the model to the data and it is measured in the units of Y. The $R^2$ statistic provides an alternative measure of fit which is independent of Y and it is in the range between 0 and 1. The $R^2$ statistic is a measure of the linear relationship between X and Y .The value close to 1 is considered as good fit. For example if the value is 0.8 which means that the linear regression model is 80% fit to the data. As compared to RSE, the $R^2$ statistic explains how well the model fit to the data.

The $R^2$ statistic has an interpretational advantage over the RSE, since unlike the RSE, it always lies between 0 and 1. However, it can still be challenging to determine what is a good $R^2$ value, and in general, this will depend on the application. For instance, in certain problems in physics, we may know that the data truly comes from a linear model with a small residual error. In this case, we would expect to see an $R^2$ value that is extremely close to 1, and a substantially smaller $R^2$ value might indicate a serious problem with the experiment in which the data were generated. On the other hand, in typical applications in biology, psychology, marketing, and other domains, the linear model is at best an extremely rough approximation to the data, and residual errors due to other unmeasured factors are often very large. In this setting, we would expect only a very small proportion of the variance in the response to be explained by the predictor, and an $R^2$ value well below 0.1 might be more realistic!

## Multiple linear regression
In multiple linear regression there are more than one predictor. For example in the Advertising data, there are three predictors such as TV, radio and newspaper. We could simply run linear regression equation for each predictor and understand how each predictor responds to sales. Looking into the table below, increase in radio advertising budget by \$1000, an average increase in sales by 203 units. Similarly increase in newspaper advertising budget by \$1000, an average increase in sales by 55 units. 

![Separate linear regression of sales on radio and newspaper](T3-3.png)

###Problem with linear regression
If three separate linear regression equations are used to predict response (as mentioned above), the results may be misleading. For example, each of the three regression equations ignores the other two media in forming estimates for the regression coefficients. If the media budgets are correlated with each other in the 200 markets that constitute our data set, then this can lead to very misleading estimates of the individual media effects on sales. A better approach to incorporate multiple predictors. The multiple regression equation for $p$ distinct predictors can be written as:
$$ Y = \beta_0+\beta_1X_1+\beta_2X_2+...+\beta_pX_p+\epsilon$$
where $X_j$ represents the $j$th predictor and $\beta_j$ quantifies the association between that variable and the response. Based on the advertising data the equation can be written as:
$$ sales = \beta_0 + \beta_1TV + \beta_2radio + \beta_3newspaper + \epsilon $$ where regression coefficients $\beta_0, \beta_1,...,\beta_p$ are unknown and must be estimated. One of the approaches to estimate coefficients is least squares approach (as we did in the linear regression). Predicting response using the estimated coefficients can be written as: 
$$\hat y = \hat\beta_0 + \hat\beta_1x_1 + \hat\beta_2x_2+...+\hat\beta_px_p$$

These coefficients can be calculated in R.

Table below displays the multiple regression coefficient estimates when TV, radio, and newspaper advertising budgets are used to predict product sales using the Advertising data. As compared to coefficient estimate of multiple linear regression with coefficient estimate of linear regression, there is a significant difference in the coefficients of TV, radio and newspaper. For \$1,000 increase in budget for TV advertising, the average sale is about 47.5 units (linear regression) and 46 units (multiple regression); for the same increase in budget for radio advertising, the average sale is about 203 units (linear regression) and 189 units (multiple regression). However by increasing \$1,000 in newspaper advertising budget, the average increased on sale is about 55 units under simple linear regression. On the contrary, multiple linear regression shows that there is no relationship between sales and newspaper. In addition, the corresponding newspaper coefficient is close to zero and so t-statistic and a higher p-value of 0.85.

![For the Advertising data, least squares coefficient estimates of the multiple linear regression of number of units sold on radio, TV, and newspaper advertising budgets.](T3-4.png)

###Coorelation matrix

###Relationship between the response and predictors





## Exercises ##

__8. This question involves the use of simple linear regression on the Auto data set.__

(a) Use the lm() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the summary() function to print the results.

```{r}
library(ISLR)
attach(Auto)
lmodel = lm(mpg~horsepower)
summary(lmodel)


```

* _Is there a relationship between the predictor and the response?_  
Yes. Based on the null hypothesis $\beta_1$ is not zero.   

* _How strong is the relationship between the predictor and the response?_  
ffdd  

* Is the relationship between the predictor and the response positive or negative?  
Negative  

* What is the predicted mpg associated with a horsepower of 98? What are the associated 95 % confidence and prediction intervals?  

__Difference between confidence interval and prediction interval__

(b) Plot the response and the predictor. Use the abline() function to display the least squares regression line.

```{r}
plot(y=mpg,x=horsepower,pch=20)
abline(lmodel,lwd=3,col='red')
```


* Diagnostic plot
* residuals plot
* rstudent()
* hatvalues()


__11. In this problem we will investigate the t-statistic for the null hypoth- esis H0 : β = 0 in simple linear regression without an intercept. To begin, we generate a predictor x and a response y as follows.__

* __set.seed(1)__
* __x=rnorm(100)__
* __y=2*x+rnorm(100)__

(a) Perform a simple linear regression of y onto x, without an intercept. Report the coefficient estimate $\hat beta$, the standard error of this coefficient estimate, and the t-statistic and p-value associated with the nul
l hypothesis $H_0$ : $\beta = 0$. Comment on these results. (You can perform regression without an intercept using the command lm(y∼x+0).)








